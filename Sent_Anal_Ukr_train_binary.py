# -*- coding: utf-8 -*-
"""Transformers.ipynb
Automatically generated by Colaboratory.
Original file is located at
    https://colab.research.google.com/drive/1-myYN08swe09digC2GtU3n-hp2V6OGN6
"""

!pip install transformers[sentencepiece]
!pip install datasets

from datasets import load_dataset

dataset = load_dataset('SergiiGurbych/sent_anal_ukr_binary')
dataset = dataset["train"]

from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("youscan/ukr-roberta-base")
tokenized_data = tokenizer(dataset['text'], return_tensors="np", padding=True)

# Берем один столбец прямо из объекта класса BatchEncoding.
tokenized_data2=tokenized_data['input_ids']

import numpy as np
labels = np.array(dataset["labels"])
labels=np.reshape(labels, (201,1))

from transformers import TFAutoModelForSequenceClassification
from tensorflow.keras.optimizers import Adam

import tensorflow as tf

# Load and compile our model
model = TFAutoModelForSequenceClassification.from_pretrained("youscan/ukr-roberta-base", from_pt=True, num_labels=2)

# Lower learning rates are often better for fine-tuning transformers
model.compile(optimizer=Adam(learning_rate=3e-5), loss='binary_crossentropy')

callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)
log=model.fit(tokenized_data2, labels, epochs=8, verbose=True, validation_split=0.2, callbacks=[callback])

import matplotlib.pyplot as plt
plt.plot(log.history['loss'])
plt.plot(log.history['val_loss'])
plt.grid(True)
plt.show()

model.save_weights('model_weights')

# predicting a new label just to check that the fine-tuned model works properly
text = "Ця потвора вкрала в неї капелюха"
text1="Я тебе кохаю, бо ти найкрасивіша у світі, і я завжди буду любити тільки тебе"
text2='Мені зовсім не подобається його поведінка в школі'
encoding = tokenizer(text, return_tensors="np", padding=True)['input_ids']
encoding1 = tokenizer(text1, return_tensors="np", padding=True)['input_ids']
encoding2 = tokenizer(text2, return_tensors="np", padding=True)['input_ids']

outputs = model.predict_on_batch(encoding2)
class_preds = np.argmax(outputs["logits"])
print(outputs)
print(class_preds)
